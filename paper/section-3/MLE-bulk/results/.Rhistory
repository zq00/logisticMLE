mean(p.t<=0.01)*100;sqrt(mean(p.t<=0.01)*(1-mean(p.t<=0.01))/B)*100
p.t
#table 7 column 1
mean(p.t<=0.1)*100; sqrt(mean(p.t<=0.1)*(1-mean(p.t<=0.1))/B)*100
mean(p.t<=0.05)*100;sqrt(mean(p.t<=0.05)*(1-mean(p.t<=0.05))/B)*100
mean(p.t<=0.01)*100;sqrt(mean(p.t<=0.01)*(1-mean(p.t<=0.01))/B)*100
mean(p.t<=0.001)*100;sqrt(mean(p.t<=0.001)*(1-mean(p.t<=0.001))/B)*100
g_pval_tr <- ggplot(p.tr) +
geom_histogram(aes(x = x),
breaks = seq(0,1,by=0.05),fill='skyblue2', color='black',alpha=0.8)+
theme(axis.text.x = element_text(color = "grey15", size = 12),
axis.text.y = element_text(color = "grey15", size = 12),
axis.title.x = element_text(color = "black", size = 15),
axis.title.y = element_text(color = "black", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))+
ylab('Density')+xlab('P-values')
filename <- "/genetic/figure/pval_tr.png"
ggsave(filename, plot = g_pval_tr, device = "png",
scale = 1, width = 4, height = 3, units = "in",
dpi = 300)
#-- Distribution of the LLR for the same null coordinate
#obtain llr
#obtain hat(\lambda)
#obtain p-value according to kappa * hat(\sigma)^2 / hat(\lambda) * chi2_1
p.val = pchisq(dev * hat_lambda / kappa / hat_sigma^2, df = 1, lower.tail=FALSE)
# table 7 column 3
mean(p.val<=0.1)*100; sqrt(mean(p.val<=0.1)*(1-mean(p.val<=0.1))/B)*100
mean(p.val<=0.05)*100;sqrt(mean(p.val<=0.05)*(1-mean(p.val<=0.05))/B)*100
mean(p.val<=0.01)*100;sqrt(mean(p.val<=0.01)*(1-mean(p.val<=0.01))/B)*100
mean(p.val<=0.001)*100;sqrt(mean(p.val<=0.001)*(1-mean(p.val<=0.001))/B)*100
#figure 4 (c):empirical cdf of adjusted p-values
ecdf_p = data.frame(cbind(sort(p.val,decreasing=FALSE),(seq(1,length(p.val),1)/(length(p.val)+1))))
colnames(ecdf_p) = c('x','y')
theme_set(theme_bw())
g_ecdf_llr <- ggplot(ecdf_p, aes(x=x, y=y))+geom_point(size=0.8) +
geom_abline(intercept=0,slope=1, linetype="dashed", color = "red")+
theme(axis.text.x = element_text(color = "grey20", size = 12),
axis.text.y = element_text(color = "grey20", size = 12),
axis.title.x = element_text(color = "grey20", size = 15),
axis.title.y = element_text(color = "grey20", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))+  ylab('Empirical cdf')+xlab('P-values')
plot(g_ecdf_llr)
filename <- "/genetic/figure/ecdf_llr.png"
ggsave(filename, plot = g_ecdf_llr, device = "png",
scale = 1, width = 4, height = 3, units = "in",
dpi = 300)
#figure 6 (b) : histogram of the p-values for the llr
p.val <- data.frame(p.val)
colnames(p.val) <- c('x')
g_pval_llr <- ggplot(p.val) +
geom_histogram(aes(x = x),
breaks = seq(0,1,by=0.05),fill='skyblue2', color='black',alpha=0.8)+
theme(axis.text.x = element_text(color = "grey15", size = 12),
axis.text.y = element_text(color = "grey15", size = 12),
axis.title.x = element_text(color = "black", size = 15),
axis.title.y = element_text(color = "black", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))+
ylab('Density')+xlab('P-values')
filename <- "/genetic/figure/g_pval_llr.png"
ggsave(filename, plot = g_pval_llr, device = "png",
scale = 1, width = 4, height = 3, units = "in",
dpi = 300)
#figure 6 (a) :  histogram of the unadjusted p-values
p.unj = pchisq(dev, df = 1, lower.tail=FALSE)
p.unj <- data.frame(p.unj)
colnames(p.unj) <- c('x')
g_pval_llr_unadj <- ggplot(p.unj) +
geom_histogram(aes(x = x),
breaks = seq(0,1,by=0.05),fill='skyblue2', color='black',alpha=0.8)+
theme(axis.text.x = element_text(color = "grey15", size = 12),
axis.text.y = element_text(color = "grey15", size = 12),
axis.title.x = element_text(color = "black", size = 15),
axis.title.y = element_text(color = "black", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))+
ylab('Density')+xlab('P-values')
filename <- "/genetic/figure/pval_llr_unadj.png"
ggsave(filename, plot = g_pval_llr_unadj, device = "png",
scale = 1, width = 4, height = 3, units = "in",
dpi = 300)
#-- Coverage
# Table 8: coverage proportion of one non-null variable
beta <- scan("/genetics/param/beta.txt")
non_null <- scan("/genetics/param/non_null.txt")
#obtain hat(\beta)
hat_beta = as.matrix(beta_est)
#obtain hat(\tau)
hat_tau = as.matrix(cov_est)/sqrt(1-kappa)
tilde_beta <- matrix(0, B, 1454)
for(i in 1:B){
tilde_beta[i,] <- hat_tau[i,] * (hat_beta[i,]-hat_alpha[i]*beta)/hat_sigma[i]
}
# randomly pick a non-null
j_non_null <- sample(non_null, 1)
p_0.99 <- mean(abs(tilde_beta[,j_non_null])<2.58);
p_0.99*100; sqrt(p_0.98*(1-p_0.98)/B)*100 #99%
p_0.98 <- mean(abs(tilde_beta[,j_non_null])<2.32);
p_0.99 <- mean(abs(tilde_beta[,j_non_null])<2.58);
p_0.99*100; sqrt(p_0.99*(1-p_0.99)/B)*100 #99%
p_0.98 <- mean(abs(tilde_beta[,j_non_null])<2.32);
p_0.98*100; sqrt(p_0.98*(1-p_0.98)/B)*100 #98%
p_0.95 <- mean(abs(tilde_beta[,j_non_null])<1.96);
p_0.95*100; sqrt(p_0.95*(1-p_0.95)/B)*100 #95%
p_0.90 <- mean(abs(tilde_beta[,j_non_null])<1.64);
p_0.90*100; sqrt(p_0.90*(1-p_0.90)/B)*100 #90%
p_0.80 <- mean(abs(tilde_beta[,j_non_null])<1.28);
p_0.80*100;sqrt(p_0.80*(1-p_0.80)/B)*100 #80%
p_0.80 <- mean(abs(tilde_beta[,j_non_null])<1.28);
p_0.80*100;sqrt(p_0.80*(1-p_0.80)/B)*100 #80%
p_0.99 <- mean(abs(tilde_beta[,j_non_null])<2.58);
p_0.99*100; sqrt(p_0.99*(1-p_0.99)/B)*100 #99%
p_0.98 <- mean(abs(tilde_beta[,j_non_null])<2.32);
p_0.98*100; sqrt(p_0.98*(1-p_0.98)/B)*100 #98%
p_0.95 <- mean(abs(tilde_beta[,j_non_null])<1.96);
p_0.95*100; sqrt(p_0.95*(1-p_0.95)/B)*100 #95%
p_0.90 <- mean(abs(tilde_beta[,j_non_null])<1.64);
p_0.90*100; sqrt(p_0.90*(1-p_0.90)/B)*100 #90%
p_0.80 <- mean(abs(tilde_beta[,j_non_null])<1.28);
p_0.80*100;sqrt(p_0.80*(1-p_0.80)/B)*100 #80%
dir
#-- Coverage
# Table 8: coverage proportion of one non-null variable
beta <- scan(paste0(dir,"/genetics/param/beta.txt"))
non_null <- scan(paste0(dir,"/genetics/param/non_null.txt"))
#obtain hat(\beta)
hat_beta = as.matrix(beta_est)
#obtain hat(\tau)
hat_tau = as.matrix(cov_est)/sqrt(1-kappa)
tilde_beta <- matrix(0, B, 1454)
for(i in 1:B){
tilde_beta[i,] <- hat_tau[i,] * (hat_beta[i,]-hat_alpha[i]*beta)/hat_sigma[i]
}
# randomly pick a non-null
j_non_null <- sample(non_null, 1)
p_0.99 <- mean(abs(tilde_beta[,j_non_null])<2.58);
p_0.99*100; sqrt(p_0.99*(1-p_0.99)/B)*100 #99%
p_0.98 <- mean(abs(tilde_beta[,j_non_null])<2.32);
p_0.98*100; sqrt(p_0.98*(1-p_0.98)/B)*100 #98%
p_0.95 <- mean(abs(tilde_beta[,j_non_null])<1.96);
p_0.95*100; sqrt(p_0.95*(1-p_0.95)/B)*100 #95%
p_0.90 <- mean(abs(tilde_beta[,j_non_null])<1.64);
p_0.90*100; sqrt(p_0.90*(1-p_0.90)/B)*100 #90%
p_0.80 <- mean(abs(tilde_beta[,j_non_null])<1.28);
p_0.80*100;sqrt(p_0.80*(1-p_0.80)/B)*100 #80%
# Figure 5
betahat_nonnull <-  data.frame(
cbind(
sort(tilde_beta[,j_non_null],decreasing=FALSE),
qnorm((seq(1,B,1)/(B+1))),0,1)
)
colnames(betahat_nonnull) = c('x','y')
g_qqplot <- ggplot(betahat_nonnull, aes(x=x, y=y))+
geom_point(size=0.8) +
geom_abline(intercept=0,slope=1, linetype="dashed", color = "red")+
theme(axis.text.x = element_text(color = "grey20", size = 12),
axis.text.y = element_text(color = "grey20", size = 12),
axis.title.x = element_text(color = "grey20", size = 15),
axis.title.y = element_text(color = "grey20", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))+
ylab('Empirical quantiles')+
xlab('Normal quantiles')
plot(g_qqplot)
filename <- "/genetics/figure/subgaussian-nonnull.png"
ggsave(filename, plot = g_qqplot, device = "png",
scale = 1, width = 4, height = 3, units = "in",
dpi = 300)
#coverage proportion overall
#coverage proportion
mean(rowMeans(abs(tilde_beta[,non_null])<2.32))*100; sd(rowMeans(abs(tilde_beta[,non_null])<2.32))/sqrt(B)*100 #98%
mean(rowMeans(abs(tilde_beta[,non_null])<1.96))*100; sd(rowMeans(abs(tilde_beta[,non_null])<1.96))/sqrt(B)*100 #95%
mean(rowMeans(abs(tilde_beta[,non_null])<1.64))*100; sd(rowMeans(abs(tilde_beta[,non_null])<1.64))/sqrt(B)*100 #90%
mean(rowMeans(abs(tilde_beta[,non_null])<1.28))*100; sd(rowMeans(abs(tilde_beta[,non_null])<1.28))/sqrt(B)*100 #80%
qnorm(0.95)
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine <- read.csv(wine.url, header = TRUE, sep = ";")
dim(wine)
wine$quality <- ifelse(wine$quality<=5, 0, 1) # "high" quality wine has score higher than 5
wine[,c(4, 6)] <- log(wine[,c(4, 6)]) # log-transform to make covariates look more normally distributed
wine[,-12] <- scale(wine[,-12], center = TRUE, scale = FALSE) # centering
p <- dim(wine)[2] - 1 # number of covariates
n <- dim(wine)[1] # number of observations
fit_full <- glm(quality~., data = wine, family = binomial(link = "logit"))
beta <- fit_full$coef
v <- 2
kappas <- c(0.02, 0.06, 0.10, 0.14, 0.18, 0.22, 0.26) # problem dimensions
ns <- round(p/kappas)
kappas <- c(0.02, 0.06, 0.10, 0.14, 0.18, 0.22, 0.26) # problem dimensions
ns <- round(p/kappas)
set.seed(2020101) # make sure we get the same answer
B <- 100 # repeat B times
mle <- matrix(0, B, length(kappas)) # fitted MLE
sd_r <- matrix(0, B, length(kappas)) # standard error from R
sd_fisher <- numeric(length(kappas)) # inverse Fisher info, plug in beta_hat from full data
for(i in 1:length(kappas)){
nn <- round(p/kappas[i])
emp <- matrix(0, p+1, p+1) # empirical Fisher information matrix
b <- 1
while(b<=B){ # retain values when the data is not separable
data <- wine[sample(1:n, nn, replace = TRUE), ]
if(!is.separable(as.matrix(data[,-12]), data[,12])){
fit <- glm(quality~., data = data, family = binomial(link = "logit"))
mle[b,i] <- coef(fit)[v+1] # fitted coefficient
sd_r[b,i] <- summary(fit)$coef[v+1,2] * sqrt(nn); # standard error from R, adjusted by sample size
# standard error from Fisher information
X <- as.matrix(cbind(1,data[,-12]));
eta <- as.vector(X%*%beta)
W <- diag(1/(1+exp(eta))/(1+exp(-eta)))
emp <- emp + t(X)%*%(W%*%X)
b <- b+1
}
}
avg=emp/B
sd_fisher[i]=sqrt(solve(avg)[v+1,v+1])* sqrt(nn) # standard error from Fisher info, adjusted by sample size
}
# Function to detect whether data is separable
is.separable <- function(X, Y){
# the code is adapted from https://rpubs.com/abhaypadda/linear-optimization-example
X <- cbind(1, X)
n <- dim(X)[1]; p <- dim(X)[2]
# the objective is sum_i a_i x_i (x_i are the variables), then this is the vector of a_i
objective.in <- colSums( X * matrix(rep(2*Y-1, each = p), n, p, byrow = TRUE))
objective.in <- c(objective.in, -objective.in)
# rhs for the constraints
const.rhs <- c(rep(0, n), rep(1, 2*p))
# create constraint martix
const.mat <- rbind(X * matrix(rep(1-2*Y, each = p), n, p, byrow = TRUE),
diag(rep(1, p)),
diag(rep(-1, p)))
const.mat <- cbind(const.mat, -const.mat)
# constraints direction
const.dir  <- rep("<=", (n + 2*p))
# find the optimal solution
optimum <- lp(direction="max",  objective.in, const.mat, const.dir,  const.rhs)
z1 <- optimum$solution[1:p]
z2 <- optimum$solution[(p+1):(2*p)]
x <- z1 - z2
ifelse(sum(x^2)<10^(-5), FALSE, TRUE)
}
B <- 100 # repeat B times
mle <- matrix(0, B, length(kappas)) # fitted MLE
sd_r <- matrix(0, B, length(kappas)) # standard error from R
sd_fisher <- numeric(length(kappas)) # inverse Fisher info, plug in beta_hat from full data
for(i in 1:length(kappas)){
nn <- round(p/kappas[i])
emp <- matrix(0, p+1, p+1) # empirical Fisher information matrix
b <- 1
while(b<=B){ # retain values when the data is not separable
data <- wine[sample(1:n, nn, replace = TRUE), ]
if(!is.separable(as.matrix(data[,-12]), data[,12])){
fit <- glm(quality~., data = data, family = binomial(link = "logit"))
mle[b,i] <- coef(fit)[v+1] # fitted coefficient
sd_r[b,i] <- summary(fit)$coef[v+1,2] * sqrt(nn); # standard error from R, adjusted by sample size
# standard error from Fisher information
X <- as.matrix(cbind(1,data[,-12]));
eta <- as.vector(X%*%beta)
W <- diag(1/(1+exp(eta))/(1+exp(-eta)))
emp <- emp + t(X)%*%(W%*%X)
b <- b+1
}
}
avg=emp/B
sd_fisher[i]=sqrt(solve(avg)[v+1,v+1])* sqrt(nn) # standard error from Fisher info, adjusted by sample size
}
library(tidyverse)
library(lpSolve)
B <- 100 # repeat B times
mle <- matrix(0, B, length(kappas)) # fitted MLE
sd_r <- matrix(0, B, length(kappas)) # standard error from R
sd_fisher <- numeric(length(kappas)) # inverse Fisher info, plug in beta_hat from full data
for(i in 1:length(kappas)){
nn <- round(p/kappas[i])
emp <- matrix(0, p+1, p+1) # empirical Fisher information matrix
b <- 1
while(b<=B){ # retain values when the data is not separable
data <- wine[sample(1:n, nn, replace = TRUE), ]
if(!is.separable(as.matrix(data[,-12]), data[,12])){
fit <- glm(quality~., data = data, family = binomial(link = "logit"))
mle[b,i] <- coef(fit)[v+1] # fitted coefficient
sd_r[b,i] <- summary(fit)$coef[v+1,2] * sqrt(nn); # standard error from R, adjusted by sample size
# standard error from Fisher information
X <- as.matrix(cbind(1,data[,-12]));
eta <- as.vector(X%*%beta)
W <- diag(1/(1+exp(eta))/(1+exp(-eta)))
emp <- emp + t(X)%*%(W%*%X)
b <- b+1
}
}
avg=emp/B
sd_fisher[i]=sqrt(solve(avg)[v+1,v+1])* sqrt(nn) # standard error from Fisher info, adjusted by sample size
}
apply(mle, 2, median)
apply(mle, 2, median)/beta[3]
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align="center",fig.width=4, fig.height = 3)
library(tidyverse)
library(lpSolve)
# Global theme
my_theme <- theme(
axis.text.x = element_text(color = "grey10", size = 12),
axis.text.y = element_text(color = "grey10", size = 12),
axis.title.x = element_text(color = "black", size = 15),
axis.title.y = element_text(color = "black", size = 15),
plot.margin = unit(c(0.3, 0.3, 0.2, 0.2), "cm"))
# Function to detect whether data is separable
is.separable <- function(X, Y){
# the code is adapted from https://rpubs.com/abhaypadda/linear-optimization-example
X <- cbind(1, X)
n <- dim(X)[1]; p <- dim(X)[2]
# the objective is sum_i a_i x_i (x_i are the variables), then this is the vector of a_i
objective.in <- colSums( X * matrix(rep(2*Y-1, each = p), n, p, byrow = TRUE))
objective.in <- c(objective.in, -objective.in)
# rhs for the constraints
const.rhs <- c(rep(0, n), rep(1, 2*p))
# create constraint martix
const.mat <- rbind(X * matrix(rep(1-2*Y, each = p), n, p, byrow = TRUE),
diag(rep(1, p)),
diag(rep(-1, p)))
const.mat <- cbind(const.mat, -const.mat)
# constraints direction
const.dir  <- rep("<=", (n + 2*p))
# find the optimal solution
optimum <- lp(direction="max",  objective.in, const.mat, const.dir,  const.rhs)
z1 <- optimum$solution[1:p]
z2 <- optimum$solution[(p+1):(2*p)]
x <- z1 - z2
ifelse(sum(x^2)<10^(-5), FALSE, TRUE)
}
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine <- read.csv(wine.url, header = TRUE, sep = ";")
wine$quality <- ifelse(wine$quality<=5, 0, 1) # "high" quality wine has score higher than 5
wine[,c(4, 6)] <- log(wine[,c(4, 6)]) # log-transform to make covariates look more normally distributed
wine[,-12] <- scale(wine[,-12], center = TRUE, scale = FALSE) # centering
p <- dim(wine)[2] - 1 # number of covariates
n <- dim(wine)[1] # number of observations
fit_full <- glm(quality~., data = wine, family = binomial(link = "logit"))
beta <- fit_full$coef
v <- 2
kappas <- c(0.02, 0.06, 0.10, 0.14, 0.18, 0.22, 0.26) # problem dimensions
ns <- round(p/kappas)
set.seed(2020101) # make sure we get the same answer
B <- 100 # repeat B times
mle <- matrix(0, B, length(kappas)) # fitted MLE
sd_r <- matrix(0, B, length(kappas)) # standard error from R
sd_fisher <- numeric(length(kappas)) # inverse Fisher info, plug in beta_hat from full data
for(i in 1:length(kappas)){
nn <- round(p/kappas[i])
emp <- matrix(0, p+1, p+1) # empirical Fisher information matrix
b <- 1
while(b<=B){ # retain values when the data is not separable
data <- wine[sample(1:n, nn, replace = TRUE), ]
if(!is.separable(as.matrix(data[,-12]), data[,12])){
fit <- glm(quality~., data = data, family = binomial(link = "logit"))
mle[b,i] <- coef(fit)[v+1] # fitted coefficient
sd_r[b,i] <- summary(fit)$coef[v+1,2] * sqrt(nn); # standard error from R, adjusted by sample size
# standard error from Fisher information
X <- as.matrix(cbind(1,data[,-12]));
eta <- as.vector(X%*%beta)
W <- diag(1/(1+exp(eta))/(1+exp(-eta)))
emp <- emp + t(X)%*%(W%*%X)
b <- b+1
}
}
avg=emp/B
sd_fisher[i]=sqrt(solve(avg)[v+1,v+1])* sqrt(nn) # standard error from Fisher info, adjusted by sample size
}
apply(mle, 2, median)
apply(mle, 2, median)/beta[3]
boxdata <- tibble(
kappa = factor(rep(kappas, each = B)),
mle = as.vector(mle),
sd_r = as.vector(sd_r)
)
g_coef <- boxdata %>%
filter(mle<20 & mle > -35) %>%
ggplot(.) +
geom_boxplot(aes(x = kappa, y=mle)) +
geom_hline(yintercept = fit_full$coefficients[v+1],linetype="longdash", color = "red", size = 0.6) +
scale_y_continuous(breaks = seq(-30, 20, by=10), limits = c(-42, 25))+
xlab(expression(kappa))+
ylab("Estimated Coefficient")+
theme_bw()+
my_theme
# add the points that has been thresholded
text1 <- boxdata %>%
filter(mle>=20) %>%
group_by(kappa) %>%
summarize(n = n(),
val = round(max(mle))) %>%
mutate(n = map_chr(n, ~paste0("[", ., "]")))
text2 <- boxdata %>%
filter(mle<=-35) %>%
group_by(kappa) %>%
summarize(n = n(),
val = round(min(mle))) %>%
mutate(n = map_chr(n, ~paste0("[", ., "]")))
# add range
g_coef <- g_coef +
geom_text(aes(x = text1$kappa, y = 20,  label = text1$n),data = text1, size = 4) +
geom_text(aes(x = text1$kappa, y = 25,  label = text1$val),data = text1, size = 4) +
geom_text(aes(x = kappa, y = -37,  label = n), data = text2, size = 4) +
geom_text(aes(x = kappa, y = -42,  label = val), data = text2, size = 4)
g_coef
# Save figure
filename <- "/Users/zq/Dropbox/logistic_cov/LogisticCov_v4/real/fig/g_coef.png"
ggsave(filename, plot = g_coef, scale = 1, width = 5, height = 3, units = "in", dpi = 300)
# n - total number of observations
# size - size of each partition
partition <- function(n, size){
nn <- floor(n / size)
matrix(sample(1:n, n, replace=FALSE)[1:(nn*size)], nn, size)
}
sd_true <- numeric(length(kappas))
for(i in 1:length(kappas)){
nn <- round(p/kappas[i])
sample_index <- partition(n, nn)
mle <- NULL
for(b in 1:dim(sample_index)[1]){
data <- wine[sample_index[b,], ]
if(!is.separable(as.matrix(data[,-12]), data[,12])){
fit <- glm(quality~., data = data, family = binomial(link = "logit"))
mle <- c(mle, coef(fit)[v+1])
}
}
sd_true[i] <- sd(mle) * sqrt(nn)
}
g_sd_r <- ggplot()+
geom_boxplot(aes(x = kappa, y=sd_r), data = boxdata %>% filter(sd_r<120)  ) +
geom_point(aes(x=factor(kappas),y = sd_true), color = "red", shape = 17, size = 2)+
xlab(expression(kappa))+
ylab("Estimated standard error")+
scale_y_continuous( breaks = c(25, 50, 75, 100, 125, 150), limits = c(10, 150))+
theme_bw()+
my_theme
# add the points that has been thresholded
text1 <- boxdata %>%
filter(sd_r >= 120) %>%
group_by(kappa) %>%
summarize(n = n(),
val = round(max(sd_r))) %>%
mutate(n = map_chr(n, ~paste0("[", ., "]")))
# add range
g_sd_r <- g_sd_r +
geom_text(aes(x = text1$kappa, y = 140,  label = text1$n),data = text1, size = 4) +
geom_text(aes(x = text1$kappa, y = 150,  label = text1$val),data = text1, size = 4)
g_sd_r
g_sd_fisher <- ggplot()+
geom_point(aes(x = factor(kappas), y=sd_fisher),color = "blue", shape = 17, size = 2) +
geom_point(aes(x=factor(kappas),y =sd_true), color = "red", shape = 17, size = 2)+
scale_y_continuous( breaks = c(25, 50, 75, 100, 125, 150), limits = c(10, 150))+
xlab(expression(kappa))+
ylab("Estimated standard error")+
theme_bw()+
my_theme
g_sd_fisher
# Save Figure
filename <- "/Users/zq/Dropbox/logistic_cov/LogisticCov_v4/real/fig/g_sd_r.png"
ggsave(filename, plot = g_sd_r, device = "png", scale = 1, width = 4, height = 3, units = "in", dpi = 300)
filename <- "/Users/zq/Dropbox/logistic_cov/LogisticCov_v4/real/fig/g_sd_fisher.png"
ggsave(filename, plot = g_sd_fisher, device = "png", scale = 1, width = 4, height = 3, units = "in", dpi = 300)
setwd("~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-3/MLE-bulk/results")
dir <- "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/"
#obtain files
type = "ar_0.5";
#obtain files
type = "ar_0.5";
B = (1:120)
#obtain all the coverage proportions
cov_t = NULL; cov_r = NULL;
dir <- "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/"
for(b in B){
newfile = paste0(dir, 'MLE-bulk/output/',type, '/pct',b,'.txt')
newcov = as.matrix(read.table(newfile, header=FALSE))
cov_t = rbind(cov_t, newcov)
}
paste0(dir, 'MLE-bulk/output/',type, '/pct',b,'.txt')
newfile = paste0(dir, 'section-3/MLE-bulk/output/',type, '/pct',b,'.txt')
newcov = as.matrix(read.table(newfile, header=FALSE))
cov_t = rbind(cov_t, newcov)
B
#obtain files
type = "ar_0.5";
B = (1:120)
#obtain all the coverage proportions
cov_t = NULL; cov_r = NULL;
dir <- "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/"
for(b in B){
newfile = paste0(dir, 'section-3/MLE-bulk/output/',type, '/pct',b,'.txt')
newcov = as.matrix(read.table(newfile, header=FALSE))
cov_t = rbind(cov_t, newcov)
}
cov_t = cov_t[1:100000,]
#compute statistics
apply(cov_t, 2, mean)*100
apply(cov_t, 2, sd)/sqrt(100000)*100
pnorm(2.676)
qnorm(0.996)
qnorm(0.995)
hat.tau.rho_1 <- read.table("~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat-tau-rho_1.txt", quote="\"", comment.char="")
View(hat.tau.rho_1)
hat.tau.rho_1 <- read.table("~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat-tau-rho_1.txt", quote="\"", comment.char="")
hat.tau.rho_1 <- read.table("~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat-tau-rho_1.txt", quote="\"", comment.char="")
hat.tau.rho_1 <- read.table("~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat-tau-rho_1.txt", quote="\"", comment.char="")
dim(hat.tau.rho_1)
dim(hat.tau.rho_1)
which((1:dim(hat.tau.rho_1)[1])%%10==i)
new <- hat.tau.rho_1[which((1:dim(hat.tau.rho_1)[1])%%10==i),]
dim(new)
write.table(new, "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat_tau_rho_",i,".txt")
?write.table
i
write.table(new, "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat_tau_rho_",i,".txt")
write.table(new, "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat_tau_rho_",i,".txt")
write.table(new, "~/Desktop/Topics/Finished projects/LogisticCov/Code/paper/section-5/output/hat_tau_rho_",i,".txt", quote = FALSE)
View(new)
